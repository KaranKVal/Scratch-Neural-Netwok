{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f99280da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Loss for epoch 0: 937.958622229471\n",
      "Epoch Loss for epoch 1: 29.157330842487237\n",
      "Epoch Loss for epoch 2: 20.917683434177867\n",
      "Epoch Loss for epoch 3: 19.358541018717364\n",
      "Epoch Loss for epoch 4: 18.1186036908682\n",
      "Epoch Loss for epoch 5: 17.177130062984176\n",
      "Epoch Loss for epoch 6: 16.468431071074612\n",
      "Epoch Loss for epoch 7: 15.925376882423365\n",
      "Epoch Loss for epoch 8: 15.49840063838708\n",
      "Epoch Loss for epoch 9: 15.153968866452026\n",
      "Epoch Loss for epoch 10: 14.869685931376276\n",
      "Epoch Loss for epoch 11: 14.630368911530365\n",
      "Epoch Loss for epoch 12: 14.425464390402349\n",
      "Epoch Loss for epoch 13: 14.247495749007317\n",
      "Epoch Loss for epoch 14: 14.091039581043136\n",
      "Epoch Loss for epoch 15: 13.9520187742865\n",
      "Epoch Loss for epoch 16: 13.827362242570903\n",
      "Epoch Loss for epoch 17: 13.714768408586513\n",
      "Epoch Loss for epoch 18: 13.612310452819585\n",
      "Epoch Loss for epoch 19: 13.518666528941651\n",
      "Epoch Loss for epoch 20: 13.432412700523164\n",
      "Epoch Loss for epoch 21: 13.35234860496766\n",
      "Epoch Loss for epoch 22: 13.28020791463353\n",
      "Epoch Loss for epoch 23: 13.212082356101812\n",
      "Epoch Loss for epoch 24: 13.148157541652598\n",
      "Epoch Loss for epoch 25: 13.088285870737437\n",
      "Epoch Loss for epoch 26: 13.031924510458492\n",
      "Epoch Loss for epoch 27: 12.978838848385182\n",
      "Epoch Loss for epoch 28: 12.928855188835316\n",
      "Epoch Loss for epoch 29: 12.881288841743462\n",
      "Epoch Loss for epoch 30: 12.836237008216898\n",
      "Epoch Loss for epoch 31: 12.793658771454327\n",
      "Epoch Loss for epoch 32: 12.75283990500678\n",
      "Epoch Loss for epoch 33: 12.714180003609501\n",
      "Epoch Loss for epoch 34: 12.677063398521007\n",
      "Epoch Loss for epoch 35: 12.641783516883187\n",
      "Epoch Loss for epoch 36: 12.607966114170559\n",
      "Epoch Loss for epoch 37: 12.575624052311705\n",
      "Epoch Loss for epoch 38: 12.54460079864344\n",
      "Epoch Loss for epoch 39: 12.514833725916203\n",
      "Epoch Loss for epoch 40: 12.486161496321937\n",
      "Epoch Loss for epoch 41: 12.458836871097747\n",
      "Epoch Loss for epoch 42: 12.431886030644113\n",
      "Epoch Loss for epoch 43: 12.40635726583826\n",
      "Epoch Loss for epoch 44: 12.38143827833907\n",
      "Epoch Loss for epoch 45: 12.357839796095329\n",
      "Epoch Loss for epoch 46: 12.334539762457718\n",
      "Epoch Loss for epoch 47: 12.312501319513434\n",
      "Epoch Loss for epoch 48: 12.291127088809287\n",
      "Epoch Loss for epoch 49: 12.270270002305828\n",
      "RMSE on dev data: 10.26050\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "NUM_FEATS = 90\n",
    "\n",
    "class Net(object):\n",
    "\t'''\n",
    "\t'''\n",
    "\n",
    "\tdef __init__(self, num_layers, num_units):\n",
    "\t\tnp.random.seed(42)\n",
    "\t\tself.num_layers = num_layers\n",
    "\t\tself.num_units = num_units\n",
    "\t\tself.biases = []\n",
    "\t\tself.weights = []\n",
    "\t\tfor i in range(num_layers):\n",
    "\t\t\tif i==0:\n",
    "\t\t\t\t# Input layer\n",
    "\t\t\t\tself.weights.append(np.random.uniform(-1, 1, size=(NUM_FEATS, self.num_units)))\t\t\t\t\n",
    "\t\t\telse:\n",
    "\t\t\t\t# Hidden layer\n",
    "\t\t\t\tself.weights.append(np.random.uniform(-1, 1, size=(self.num_units, self.num_units)))\n",
    "\t\t\tself.biases.append(np.random.uniform(-1, 1, size=(1, self.num_units)))\n",
    "\t\t# Output layer\n",
    "\t\tself.biases.append(np.random.uniform(-1, 1, size=(1, 1)))\n",
    "\t\tself.weights.append(np.random.uniform(-1, 1, size=(self.num_units, 1)))\n",
    "\tdef __call__(self, X):\n",
    "\t\tweights = self.weights\n",
    "\t\tbiases = self.biases\n",
    "\t\tself.activations = []\n",
    "\t\tself.dotz = []\n",
    "\t\tself.activations.append(X)\n",
    "\t\tfor layer_num in range(self.num_layers):\n",
    "\t\t\tdotz = np.dot(X, weights[layer_num]) + biases[layer_num]\n",
    "\t\t\tself.dotz.append(dotz)\n",
    "\t\t\tactivation = np.maximum(dotz, 0)\n",
    "\t\t\tself.activations.append(activation)\n",
    "\t\t\tX = activation\n",
    "\t\tdotz_out = np.dot(X, weights[-1]) + biases[-1]\n",
    "\t\tself.dotz.append(dotz_out)\n",
    "\t\ty_hat = dotz_out\n",
    "\t\tself.activations.append(y_hat)\n",
    "\t\treturn y_hat\n",
    "\t\traise NotImplementedError\n",
    "\tdef backward(self, X, y, lamda):\n",
    "\t\tdel_W = []\n",
    "\t\tdel_b = []\n",
    "\t\tm = y.shape[0]\n",
    "\t\tL = self.num_layers\n",
    "\t\ta_list = self.activations\n",
    "\t\tz_list = self.dotz\n",
    "\t\tweights = self.weights\n",
    "\t\tbiases = self.biases\n",
    "\t\ty = np.reshape(y, (y.shape[0], 1))\n",
    "\t\tdel_aL = (2/m) * (a_list[-1] - y)\n",
    "\t\tdel_WL = np.dot(a_list[-2].T, (del_aL)) + lamda * (weights[-1])\n",
    "\t\tdel_bL = np.sum(del_aL, axis=0) + lamda * (biases[-1])\n",
    "\t\tdel_W.append(del_WL)\n",
    "\t\tdel_b.append(del_bL)\n",
    "\t\t\n",
    "\t\tdel_al = del_aL\n",
    "\t\tfor l in reversed(range(1, L+1)):\n",
    "\t\t\tdel_al = np.dot((del_al), weights[l].T)\n",
    "\t\t\tdel_Wl = np.dot(a_list[l-1].T, (del_al*(z_list[l-1] > 0))) + lamda * (weights[l-1])\n",
    "\t\t\tdel_bl = np.sum(del_al*(z_list[l-1] > 0), axis=0) + lamda * (biases[l-1])\n",
    "\t\t\tdel_W.append(del_Wl)\n",
    "\t\t\tdel_b.append(del_bl)\t\n",
    "\t\tdel_W = list(reversed(del_W))\n",
    "\t\tdel_b = list(reversed(del_b))\n",
    "\t\treturn del_W, del_b\n",
    "\t\traise NotImplementedError\n",
    "class Optimizer(object):\n",
    "\tdef __init__(self, learning_rate):\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.delta_weights = []\n",
    "\t\tself.delta_biases = []\n",
    "\t\t#raise NotImplementedError\n",
    "\tdef step(self, weights, biases, delta_weights, delta_biases):\n",
    "\t\tlr = self.learning_rate\n",
    "\t\tfor layer_num in range(len(weights)):\n",
    "\t\t\tweights[layer_num] -= lr * delta_weights[layer_num]\n",
    "\t\t\tbiases[layer_num] -= lr * delta_biases[layer_num]\n",
    "\t\treturn weights, biases\n",
    "\t\traise NotImplementedError\n",
    "\n",
    "\n",
    "def loss_mse(y, y_hat):\n",
    "\n",
    "\tm = y.shape[0]\n",
    "\ty = np.reshape(y, (y.shape[0], 1))\n",
    "\tmse = (1/m)*np.sum((y - y_hat)**2)\n",
    "\treturn mse\n",
    "\traise NotImplementedError\n",
    "def loss_regularization(weights, biases):\n",
    "\trunning_sum = 0\n",
    "\tfor layer_num in range(len(weights)):\n",
    "\t\trunning_sum += np.sum((weights[layer_num])**2) + np.sum((biases[layer_num])**2)\n",
    "\treturn running_sum\n",
    "\traise NotImplementedError\n",
    "def loss_fn(y, y_hat, weights, biases, lamda):\n",
    "\t\n",
    "\tl2_loss = loss_mse(y, y_hat) + lamda * loss_regularization(weights, biases)\n",
    "\treturn l2_loss\n",
    "\traise NotImplementedError\n",
    "\n",
    "def rmse(y, y_hat):\n",
    "\trsme = (loss_mse(y, y_hat))**0.5\n",
    "\treturn rsme\n",
    "\traise NotImplementedError\n",
    "\n",
    "\n",
    "def train(\n",
    "\tnet, optimizer, lamda, batch_size, max_epochs,\n",
    "\ttrain_input, train_target,\n",
    "\tdev_input, dev_target\n",
    "):\n",
    "\tm = train_input.shape[0]\n",
    "\tfor e in range(max_epochs):\n",
    "\t\tepoch_loss = 0.\n",
    "\t\tepoch_loss_rmse = 0.\n",
    "\t\titer_count = 0\n",
    "\t\tfor i in range(0, m, batch_size):\n",
    "\t\t\titer_count += 1\n",
    "\n",
    "\t\t\tbatch_input = train_input[i:i+batch_size]\n",
    "\t\t\tbatch_target = train_target[i:i+batch_size]\n",
    "\n",
    "\t\t\tpred = net(batch_input)\n",
    "\n",
    "\t\t\t# Compute gradients of loss w.r.t. weights and biases\n",
    "\t\t\tdW, db = net.backward(batch_input, batch_target, lamda)\n",
    "\n",
    "\t\t\t#norm_dW = [np.sum(grad_mat**2) for grad_mat in dW]\n",
    "\t\t\t#norm_db = [np.sum(grad_mat**2) for grad_mat in db]\n",
    "\n",
    "\t\t\t#if iter_count%200 == 0:\n",
    "\t\t\t#\tprint('norm of gradients')\n",
    "\t\t\t#\tprint(norm_dW, norm_db)\n",
    "\t\t\t# Get updated weights based on current weights and gradients\n",
    "\t\t\tweights_updated, biases_updated = optimizer.step(net.weights, net.biases, dW, db)\n",
    "\n",
    "\t\t\t#norm_W = [np.sum(weight_mat**2) for weight_mat in weights_updated]\n",
    "\t\t\t#norm_b = [np.sum(bias_mat**2) for bias_mat in biases_updated]\n",
    "\n",
    "\t\t\t#if iter_count%200 == 0:\n",
    "\t\t\t#\tprint('norm of weights and biases')\n",
    "\t\t\t#\tprint(norm_W, norm_b)\n",
    "\t\t\t# Update model's weights and biases\n",
    "\t\t\tnet.weights = weights_updated\n",
    "\t\t\tnet.biases = biases_updated\n",
    "\n",
    "\t\t\t# Compute loss for the batch\n",
    "\t\t\tbatch_loss = loss_fn(batch_target, pred, net.weights, net.biases, lamda)\n",
    "\t\t\tepoch_loss += batch_loss\n",
    "\t\t\tbatch_loss_rmse = rmse(batch_target, pred)\n",
    "\t\t\tepoch_loss_rmse += (batch_loss_rmse)**2\n",
    "\n",
    "\t\n",
    "\n",
    "\t\tprint(f'Epoch Loss for epoch {e}: {(epoch_loss_rmse*(batch_size/m))**0.5}')\n",
    "\n",
    "\n",
    "\tdev_pred = net(dev_input)\n",
    "\tdev_rmse = rmse(dev_target, dev_pred)\n",
    "\n",
    "\tprint('RMSE on dev data: {:.5f}'.format(dev_rmse))\n",
    "\n",
    "\n",
    "def get_test_data_predictions(net, inputs):\n",
    "\t\n",
    "\ttest_preds = np.float32(np.round(net(inputs)))\n",
    "\n",
    "\tIds = np.arange(1, test_preds.shape[0] + 1, 1, dtype='f')\n",
    "\tIds = np.reshape(Ids, (Ids.shape[0], 1))\n",
    "\t\n",
    "\tpredictions = np.concatenate((Ids, test_preds), axis=1)\n",
    "\n",
    "\ttest_data_predictions = pd.DataFrame(predictions, columns=['Id', 'Predicted'])\n",
    "\ttest_data_predictions.to_csv('193109010.csv', index=False)\n",
    "\treturn test_preds\n",
    "\traise NotImplementedError\n",
    "\n",
    "def read_data():\n",
    "\tdf1 = pd.read_csv('train.csv')\n",
    "\ttrain_input = df1.iloc[:, 1:].to_numpy()\n",
    "\tindices = np.arange(0, train_input.shape[0])\n",
    "\tnp.random.shuffle(indices)\n",
    "\ttrain_input = train_input[indices]\n",
    "\ttrain_target = df1.iloc[:, 0].to_numpy()\n",
    "\ttrain_target = train_target[indices]\n",
    "\n",
    "\n",
    "\tdf2 = pd.read_csv('dev.csv')\n",
    "\tdev_input = df2.iloc[:, 1:].to_numpy()\n",
    "\tdev_target = df2.iloc[:, 0].to_numpy()\n",
    "\n",
    "\t\n",
    "\tdf3 = pd.read_csv('test.csv')\n",
    "\ttest_input = df3.iloc[:, 0:].to_numpy()\n",
    "\n",
    "\treturn train_input, train_target, dev_input, dev_target, test_input\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "\t# These parameters should be fixed for Part 1\n",
    "\tmax_epochs = 50\n",
    "\tbatch_size = 128\n",
    "\n",
    "\n",
    "\tlearning_rate = 0.001\n",
    "\tnum_layers = 1\n",
    "\tnum_units = 64\n",
    "\tlamda = 0.0 # Regularization Parameter\n",
    "\n",
    "\ttrain_input, train_target, dev_input, dev_target, test_input = read_data()\n",
    "\tnet = Net(num_layers, num_units)\n",
    "\toptimizer = Optimizer(learning_rate)\n",
    "\ttrain(\n",
    "\t\tnet, optimizer, lamda, batch_size, max_epochs,\n",
    "\t\ttrain_input, train_target,\n",
    "\t\tdev_input, dev_target\n",
    "\t)\n",
    "\t#get_test_data_predictions(net, test_input)\n",
    "\t\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a8599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ac0ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
